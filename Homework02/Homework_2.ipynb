{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Assignment: Homework 2 - Uncertainty\\\n",
    "Name:Hamin Hong\\\n",
    "Instructor: Keith Vander Linden\\\n",
    "Course: CS 344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "**Graham argues that this is a Baysian approach to SPAM. What makes it Bayesian?**\n",
    "\n",
    "It makes it Bayesian because it calculates actual probablilty to filter out the spam mail based on the total probability\n",
    "of words from past emails. It does not score individual emails and determine spam or not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam filter based on Graham's algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['i', 'am', 'spam', 'spam', 'i', 'am', 'i', 'do', 'not', 'like', 'that', 'spamiam']\n['do', 'i', 'like', 'green', 'eggs', 'and', 'ham', 'i', 'do']\ntoken_map: \n{'like': 1, 'that': 0, 'do': 2, 'ham': 1, 'spamiam': 0, 'spam': 0, 'not': 0, 'green': 1, 'and': 1, 'am': 0, 'eggs': 1, 'i': 2}\ntoken_map: \n{'like': 1, 'that': 1, 'do': 1, 'ham': 0, 'spamiam': 1, 'spam': 2, 'not': 1, 'green': 0, 'and': 0, 'am': 2, 'eggs': 0, 'i': 3}\nprobability_map: \n{'like': 0.25, 'that': 0.99, 'do': 0.25, 'ham': 0.01, 'spamiam': 0.99, 'spam': 0.99, 'not': 0.99, 'green': 0.01, 'and': 0.01, 'am': 0.99, 'eggs': 0.01, 'i': 0.5}\nTest corpus 1: \n0.999996521702835\nTest corpus 2: \n0.11636363636363642\nTest corpus 3: \n0.006688963210702337\nSpam corpus: \n0.9999999765770426\nHam corpus: \n1.1566892827567553e-09\nUnknown word: \n0.4\n"
    }
   ],
   "source": [
    "\n",
    "# Spam corpus contains spam e-mails.\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"], [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "# Ham corpus contains non-spam e-mails.\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "# Test corpus\n",
    "test_corpus = [[\"I\" \"like\" \"Green\", \"eggs\", \"and\", \"spam\"], [\"Ham\", \"is\", \"the\", \"best\", \"food\"],\n",
    "               [\"I\", \"am\", \"who\", \"I\", \"am\", \"not\", \"green\", \"spam\"]]\n",
    "\n",
    "spam_threshhold = 0.9\n",
    "interesting_tokens_num = 15\n",
    "threshhold = 1\n",
    "\n",
    "# has to be all lowercase \n",
    "def token_creater(corpus):\n",
    "    tokens = [token.lower() for message in corpus for token in message]\n",
    "\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "# count number of tokens in the message\n",
    "def token_counter(message, tokens):\n",
    "    token_count_map = {}\n",
    "    for token in tokens:\n",
    "        token_count_map[token] = message.count(token)\n",
    "\n",
    "    print(\"token_map: \")\n",
    "    print(token_count_map)\n",
    "    return token_count_map\n",
    "\n",
    "# Creates dictionary of probability of individual tokens\n",
    "def spam_probability_map(gtoken_occurences_map, btoken_occurences_map, good_num, bad_num, tokens):\n",
    "    probability_map = {}\n",
    "    for token in tokens:\n",
    "        good = float(2 * gtoken_occurences_map[token])\n",
    "        bad = float(btoken_occurences_map[token])\n",
    "        if (good + bad) > 0.9:\n",
    "            probability_map[token] = max(0.01, min(0.99, min(1.0, bad / bad_num) / (min(1.0, good / good_num) + min(1.0, bad / bad_num))))\n",
    "        else:\n",
    "            probability_map[token] = 0\n",
    "\n",
    "    print (\"probability_map: \")\n",
    "    print (probability_map)\n",
    "    return probability_map\n",
    "\n",
    "# Calculate final probability from the probability maps\n",
    "def spam_filter(corpus, prob_map):\n",
    "    product = 1.0\n",
    "    complement_product = 1.0\n",
    "    for token in corpus:\n",
    "        if token in prob_map:\n",
    "            probability = prob_map[token]\n",
    "        else:\n",
    "            probability = 0.4\n",
    "\n",
    "        product *= probability\n",
    "        complement_product *= (1.0 - probability)\n",
    "\n",
    "    return product / (product + complement_product)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    b_num = len(test_corpus)\n",
    "    g_num = len(ham_corpus)\n",
    "    spam_wordlist = token_creater(spam_corpus)\n",
    "    norm_wordlist = token_creater(ham_corpus)\n",
    "    tokens = list(set(norm_wordlist) | set(spam_wordlist))\n",
    "    norm_map = token_counter(norm_wordlist, tokens)\n",
    "    spam_map = token_counter(spam_wordlist, tokens)\n",
    "    prob_map = spam_probability_map(norm_map, spam_map, g_num, b_num, tokens)\n",
    "\n",
    "    print(\"Test corpus 1: \")\n",
    "    print(spam_filter(test_corpus[2], prob_map))\n",
    "    print(\"Test corpus 2: \")\n",
    "    print(spam_filter(test_corpus[1], prob_map))\n",
    "    print(\"Test corpus 3: \")\n",
    "    print(spam_filter(test_corpus[0], prob_map))\n",
    "    print(\"Spam corpus: \")\n",
    "    print(spam_filter(spam_corpus[0], prob_map))\n",
    "    print(\"Ham corpus: \")\n",
    "    print(spam_filter(ham_corpus[0], prob_map))\n",
    "    print(\"Unknown word: \")\n",
    "    print(spam_filter([\"low\"], prob_map))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b \n",
    " \n",
    "    2^4 = 16\n",
    "\n",
    "## 2.c \n",
    "     9 values.\n",
    "\n",
    "## 2.d \n",
    "- P(Cloudy) = <T=.5,F=.5> \n",
    "\n",
    "- P(Sprinker | cloudy) <0.1,0.9>\n",
    "\n",
    "- P(Cloudy| sprinkler, ¬rain) = α P(cloudy, sprinkler, ¬rain) = <br>\n",
    "  α <P(S|C)* P(¬R|C)* (P(C), P(S|¬C)* P(¬R|¬C)* P(¬C)> =\n",
    "  α<.1* .2* .5 , .5* .8* .5> = α<.01 , .2> = <.048 , .952>\n",
    "\n",
    "\n",
    "- P(WetGrass | cloudy, sprinkler, rain) = α P(wetgrass, cloudy, sprinkler, rain)\n",
    "  α <P(W | S,R) * P(S|C) * P(R|C) * P(C) , P(¬W | S,R) * P(S|C) * P(R|C) * P(C)> = \n",
    "  α < .99* .10* .80* .5 , .01* .10* .80* .5> = α <.0396 , .0004 > = <.99 , .01>\n",
    "\n",
    "- P(Cloudy | ¬wetgrass) = α P(Cloudy,¬wetgrass, rain, sprinkler)<br>\n",
    "  α <P(C) * P(R|C) * P(S|C)* P(¬W | S,R) + P(C) * P(¬R|C) * P(S|C) * P(¬W | S,¬R) <br>\n",
    "  +P(C) * P(R|C) * P(¬S|C) * P(¬W | ¬S,R)  + P(C) * P(¬R|C) * P(¬S|C) * P(¬W | ¬S,¬R) , <br>\n",
    "  P(¬C) * P(R|¬C) * P(S|¬C) * P(¬W | S,R) + P(¬C) * P(¬R|¬C) * P(S|¬C) * P(¬W | S,¬R) <br>\n",
    "  +P(¬C) * P(R|¬C) * P(¬S|¬C) * P(¬W | ¬S,R) +P(¬C) * P(¬R|¬C) * P(¬S|¬C) * P(¬W | ¬S,¬R)><br>\n",
    "  α <.5* .8* .1* .01 + .5* .2* .1* .1 + .5* .8* .9* .1 + .5* .2* .9* 1 ,\n",
    "  .5 *.2 *.5 *.01 + .5 *.8 *.5 *.1 + .5 *.2 *.5 *.1 + .5 *.8 *.5 *1><br>\n",
    "  α <.1272,.2255> = <.361,.639>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False: 0.5, True: 0.5\nFalse: 0.9, True: 0.1\nFalse: 0.952, True: 0.0476\nFalse: 0.01, True: 0.99\nFalse: 0.639, True: 0.361\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 2.a\n",
    "Name: Won Seok Park\n",
    "Date: Mar/15/2020\n",
    "\"\"\"\n",
    "\n",
    "from aima.probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "# From AIMA code (probability.py) - Fig. 14.2 - burglary example\n",
    "weather = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T:0.1, F:0.5}),\n",
    "    ('Rain', 'Cloudy', {T:0.8, F:0.2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T):0.99, (T,F):0.9, (F,T):0.9, (F,F):0.0}),\n",
    "    ])\n",
    "\n",
    "\n",
    "# P(Cloudy)\n",
    "print(enumeration_ask('Cloudy', dict(), weather).show_approx())\n",
    "# False: 0.5, True: 0.5 \n",
    "\n",
    "# P(Sprinker | cloudy)\n",
    "print(enumeration_ask('Sprinkler', dict(Cloudy=T), weather).show_approx())\n",
    "# False: 0.9, True: 0.1\n",
    "\n",
    "# P(Cloudy| the sprinkler is running and it’s not raining)\n",
    "print(enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), weather).show_approx())\n",
    "# False: 0.952, True: 0.0476\n",
    "\n",
    "# P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
    "print(enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), weather).show_approx())\n",
    "# False: 0.01, True: 0.99\n",
    "\n",
    "# P(Cloudy | the grass is not wet)\n",
    "print(enumeration_ask('Cloudy', dict(WetGrass=F), weather).show_approx())\n",
    "# False: 0.639, True: 0.361 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}